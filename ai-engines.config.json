{
  "default": "mock-default",
  "engines": [
    {
      "id": "mock-default",
      "provider": "mock",
      "description": "Local mock provider for testing",
      "params": {
        "response": "This is a mock response",
        "maxTokens": 100
      }
    },
    {
      "id": "openai-text-davinci",
      "provider": "openai",
      "description": "OpenAI GPT model (example)",
      "params": {
        "model": "gpt-4o-mini",
        "temperature": 0.7,
        "maxTokens": 1024
      }
    },
    {
      "id": "claude-1",
      "provider": "claude",
      "description": "Anthropic Claude example",
      "params": { "model": "claude-2", "temperature": 0.2 }
    },
    {
      "id": "gemini-1",
      "provider": "gemini",
      "description": "Google Gemini example",
      "params": { "model": "gemini-pro", "temperature": 0.3 }
    },
    {
      "id": "grok-1",
      "provider": "grok",
      "description": "Grok example",
      "params": { "model": "grok-alpha" }
    },
    {
      "id": "mistral-1",
      "provider": "mistral",
      "description": "Mistral example",
      "params": { "model": "mistral-large" }
    },
    {
      "id": "deepseek-1",
      "provider": "deepseek",
      "description": "Deepseek example",
      "params": { "model": "deepseek-base" }
    },
    {
      "id": "llama-local",
      "provider": "llama_cpp",
      "description": "Local LLaMA.cpp instance",
      "params": { "modelPath": "/models/llama.bin", "nCtx": 2048 }
    }
  ]
}
